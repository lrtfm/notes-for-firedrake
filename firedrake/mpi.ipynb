{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 多进程并行 (MPI)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在终端使用 `mpiexec -n <np>` 运行 python 文件即可:\n",
    "    \n",
    "```bash\n",
    "mpiexec -n 2 python myscript.py\n",
    "```\n",
    "\n",
    "\n",
    "<!--\n",
    "```bash\n",
    "MPICH_NO_LOCAL=1 mpiexec -n 16 -bind-to core -map-by socket python /path/to/script.py\n",
    "```\n",
    "-->"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run code parallelly in jupyter"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们使用 `ipyparallel` 介绍并行程序的一些内容, 需要先安装 `ipyparallel`\n",
    "\n",
    "### Install ipyparallel\n",
    "1. Install ipyparallel in firedrake env:\n",
    "    \n",
    "    ```bash\n",
    "    pip install ipyparallel\n",
    "    ```\n",
    "\n",
    "2. create profile mpi\n",
    "    \n",
    "    ```bash\n",
    "    ipython profile create --parallel --profile=mpi\n",
    "    ```\n",
    "    \n",
    "    Your will see the following output\n",
    "    \n",
    "    ```bash\n",
    "    [ProfileCreate] Generating default config file: \n",
    "        PosixPath('/home/<your-user-name>/.ipython/profile_mpi/ipython_config.py')\n",
    "    [ProfileCreate] Generating default config file: \n",
    "        PosixPath('/home/<your-user-name>/.ipython/profile_mpi/ipython_kernel_config.py')\n",
    "    [ProfileCreate] Generating default config file: \n",
    "        PosixPath('/home/<your-user-name>/.ipython/profile_mpi/ipcontroller_config.py')\n",
    "    [ProfileCreate] Generating default config file: \n",
    "        PosixPath('/home/<your-user-name>/.ipython/profile_mpi/ipengine_config.py')\n",
    "    [ProfileCreate] Generating default config file: \n",
    "        PosixPath('/home/<your-user-name>/.ipython/profile_mpi/ipcluster_config.py')\n",
    "    ```\n",
    "\n",
    "3. Edit file `.ipython/profile_mpi/ipengine_config.py`. Add the following code at the begining of the file:\n",
    "    \n",
    "    ```python\n",
    "    from firedrake import *\n",
    "    from firedrake.petsc import PETSc\n",
    "    ```\n",
    "\n",
    "4. Set the default engines to mpi in file `.ipython/profile_mpi/ipcluster_config.py`. \n",
    "   You can search `engine_launcher_class` in the file, and the result file should looks like this:\n",
    "    \n",
    "    ```python\n",
    "    #    - sshproxy: ipyparallel.cluster.launcher.SSHProxyEngineSetLauncher\n",
    "    #    - winhpc: ipyparallel.cluster.launcher.WindowsHPCEngineSetLauncher\n",
    "    #  Default: 'ipyparallel.cluster.launcher.LocalEngineSetLauncher'\n",
    "    c.Cluster.engine_launcher_class = 'mpi'\n",
    "    ```\n",
    "\n",
    "4. Test:\n",
    "    \n",
    "    ```python\n",
    "    import ipyparallel as ipp\n",
    "    import os\n",
    "\n",
    "    cluster = ipp.Cluster(profile=\"mpi\", n=2)\n",
    "    client = cluster.start_and_connect_sync()\n",
    "    ```\n",
    "    \n",
    "    The output should looks like\n",
    "    \n",
    "    ```bash\n",
    "    Starting 2 engines with <class 'ipyparallel.cluster.launcher.MPIEngineSetLauncher'>\n",
    "    ```\n",
    "    \n",
    "    ```python\n",
    "    %%px --block\n",
    "    from firedrake import *\n",
    "    from firedrake.petsc import PETSc\n",
    "    from mpi4py import MPI\n",
    "\n",
    "    mesh = RectangleMesh(8, 8, 1, 1)\n",
    "    PETSc.Sys.syncPrint(mesh.comm.rank, mesh.comm.size)\n",
    "    PETSc.Sys.syncFlush()\n",
    "    ```\n",
    "    \n",
    "    The output should looks like:\n",
    "    \n",
    "    ```bash\n",
    "    [stdout:0] 0 2\n",
    "    1 2\n",
    "    ```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting 2 engines with <class 'ipyparallel.cluster.launcher.MPIEngineSetLauncher'>\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3ea180a5f2b4a9981ec2672e6f822a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?engine/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ipyparallel as ipp\n",
    "import os\n",
    "\n",
    "cluster = ipp.Cluster(profile=\"mpi\", n=2)\n",
    "client = cluster.start_and_connect_sync()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[stdout:0] 0 2\n",
       "1 2\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%px --block\n",
    "from firedrake import *\n",
    "from firedrake.petsc import PETSc\n",
    "from mpi4py import MPI\n",
    "\n",
    "mesh = RectangleMesh(8, 8, 1, 1)\n",
    "PETSc.Sys.syncPrint(mesh.comm.rank, mesh.comm.size)\n",
    "PETSc.Sys.syncFlush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[stdout:0] 0 2\n",
       "1 2\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%px --block\n",
    "\n",
    "PETSc.Sys.syncPrint(COMM_WORLD.rank, COMM_WORLD.size)\n",
    "PETSc.Sys.syncFlush()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "有些时候需要在某个进程上, 做指定的操作或运算, 如只在第0个进程上画图\n",
    "\n",
    "```python\n",
    "if COMM_WORLD.rank == 0:\n",
    "    plot(...)\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 并行输出\n",
    "\n",
    "[py/intro_utils.py](py/intro_utils.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[stdout:0] This is first line (from rank 0)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%px --block \n",
    "from firedrake import *\n",
    "from firedrake.petsc import PETSc\n",
    "from mpi4py import MPI\n",
    "\n",
    "PETSc.Sys.Print('This is first line (from rank 0)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[stdout:0] This is second line (from all rank)\n",
       "This is second line (from all rank)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%px --block \n",
    "PETSc.Sys.syncPrint('This is second line (from all rank)')\n",
    "PETSc.Sys.syncFlush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[stdout:0] This msg from all rank\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[stdout:1] This msg from all rank\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%px --block\n",
    "print('This msg from all rank')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
